#!/usr/bin/env python3
"""
CDIF: CCTV-informed Diffusion-based Path Planning for Social Context-Aware Robot Navigation

í•µì‹¬ í˜ì‹ :
1. í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ì˜ ì „ëµì  í™œìš©ì„ í†µí•œ ì œë¡œ-ë¹„ìš© ì„¼ì„œ í™•ì¥
2. ì •ì  ì¥ì• ë¬¼ê³¼ ë™ì  ì‚¬íšŒì  ìš”ì†Œë¥¼ ë‹¨ì¼ í‘œí˜„ ê³µê°„ìœ¼ë¡œ í†µí•©í•˜ëŠ” í™˜ê²½ ëª¨ë¸ë§
3. ResNet ê¸°ë°˜ ì‹œê° íŠ¹ì§• ì¶”ì¶œê¸°ì™€ ì¡°ê±´ë¶€ í™•ì‚° ìƒì„± ëª¨ë¸ì„ í†µí•œ ë‹¤ì¤‘ ê²½ë¡œ í•©ì„±
4. í™•ë¥ ì  ê¸€ë¡œë²Œ ê³„íšê³¼ ë°˜ì‘ì  ë¡œì»¬ ì œì–´ì˜ ê³„ì¸µì  ìœµí•©

ì°¨ë³„í™” í¬ì¸íŠ¸:
- vs CGIP: ê²°ì •ì  â†’ í™•ë¥ ì  ë‹¤ì¤‘ ê²½ë¡œ ìƒì„±
- vs DiPPeR-Legged: ì¥ì• ë¬¼ íšŒí”¼ â†’ ì‚¬íšŒì  ë§¥ë½ ì¸ì‹
- ì‹¤ì‹œê°„ ì ì‘í˜• ë‚´ë¹„ê²Œì´ì…˜ êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, List, Optional
import math

class IntegratedCostMapEncoder(nn.Module):
    """í†µí•© ë¹„ìš© ì§€ë„ ì¸ì½”ë” - ì •ì  ì¥ì• ë¬¼ê³¼ ë™ì  ì‚¬íšŒì  ìš”ì†Œë¥¼ ë‹¨ì¼ í‘œí˜„ ê³µê°„ìœ¼ë¡œ í†µí•©"""
    
    def __init__(self, input_channels=3, feature_dim=256):  # 3ì±„ë„: ì •ì ë§µ + ì‚¬íšŒì ë¹„ìš© + íë¦„ë§µ
        super().__init__()
        
        # ë‹¤ì¤‘ ì±„ë„ íŠ¹ì§• ì¶”ì¶œ (í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ í™œìš©)
        self.static_conv = nn.Conv2d(1, 32, 3, padding=1)  # ì •ì  ì¥ì• ë¬¼
        self.social_conv = nn.Conv2d(1, 32, 3, padding=1)  # ì‚¬íšŒì  ë¹„ìš©
        self.flow_conv = nn.Conv2d(1, 32, 3, padding=1)    # ë³´í–‰ì íë¦„
        
        # í†µí•© íŠ¹ì§• ìœµí•©
        self.fusion_conv = nn.Conv2d(96, 64, 1)  # 3*32 = 96 ì±„ë„ ìœµí•©
        
        # ResNet-18 ìŠ¤íƒ€ì¼ ë°±ë³¸ (í†µí•©ëœ íŠ¹ì§• ì²˜ë¦¬)
        self.conv1 = nn.Conv2d(64, 64, 7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        
        # ResNet ë¸”ë¡ë“¤ (ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ì„ ìœ„í•œ ê¹Šì€ íŠ¹ì§• ì¶”ì¶œ)
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        
        # ê³µê°„ ì¸ì‹ í’€ë§ (ìœ„ì¹˜ ì •ë³´ ë³´ì¡´)
        self.spatial_pool = nn.AdaptiveAvgPool2d((4, 4))  # 2x2 â†’ 4x4ë¡œ í™•ì¥
        
        # íŠ¹ì§• ì••ì¶• (ì‚¬íšŒì  ë§¥ë½ ì •ë³´ ë³´ì¡´)
        self.feature_proj = nn.Sequential(
            nn.Linear(512 * 16, feature_dim * 2),  # 4x4 = 16
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU()
        )
        
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        
        # ì²« ë²ˆì§¸ ë¸”ë¡ (stride ì ìš©)
        layers.append(BasicBlock(in_channels, out_channels, stride))
        
        # ë‚˜ë¨¸ì§€ ë¸”ë¡ë“¤
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels))
            
        return nn.Sequential(*layers)
    
    def forward(self, integrated_cost_map):
        # integrated_cost_map: [B, 3, 60, 60] - ì •ì ë§µ + ì‚¬íšŒì ë¹„ìš© + íë¦„ë§µ
        
        # ì±„ë„ë³„ íŠ¹ì§• ì¶”ì¶œ (í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ì˜ ì „ëµì  í™œìš©)
        static_features = self.relu(self.static_conv(integrated_cost_map[:, 0:1]))    # ì •ì  ì¥ì• ë¬¼
        social_features = self.relu(self.social_conv(integrated_cost_map[:, 1:2]))    # ì‚¬íšŒì  ë¹„ìš©
        flow_features = self.relu(self.flow_conv(integrated_cost_map[:, 2:3]))        # ë³´í–‰ì íë¦„
        
        # ë‹¤ì¤‘ ëª¨ë‹¬ íŠ¹ì§• ìœµí•© (ì •ì  + ë™ì  ì‚¬íšŒì  ìš”ì†Œ í†µí•©)
        fused_features = torch.cat([static_features, social_features, flow_features], dim=1)
        x = self.relu(self.fusion_conv(fused_features))
        
        # ResNet ë°±ë³¸ì„ í†µí•œ ê¹Šì€ ì‚¬íšŒì  ë§¥ë½ ì¸ì‹
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # ê³µê°„ ì •ë³´ ë³´ì¡´ í’€ë§
        x = self.spatial_pool(x)  # [B, 512, 4, 4]
        x = torch.flatten(x, 1)   # [B, 512*16]
        
        # ì‚¬íšŒì  ë§¥ë½ì´ í†µí•©ëœ íŠ¹ì§• ë²¡í„°
        x = self.feature_proj(x)
        
        return x

class BasicBlock(nn.Module):
    """ResNet Basic Block"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class SinusoidalPositionEmbedding(nn.Module):
    """Sinusoidal Position Embedding for Diffusion Timesteps"""
    
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        
    def forward(self, timesteps):
        device = timesteps.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = timesteps[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings

class SocialContextAwareDiffusionNet(nn.Module):
    """ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ í™•ì‚° ë„¤íŠ¸ì›Œí¬ - ë‹¤ì¤‘ ê²½ë¡œ í›„ë³´ ë™ì‹œ ìƒì„±"""
    
    def __init__(self, 
                 max_waypoints=8,
                 feature_dim=256,
                 hidden_dim=512,
                 num_layers=6,
                 num_path_modes=3):  # ë‹¤ì¤‘ ëª¨ë‹¬ ê²½ë¡œ ìƒì„±
        super().__init__()
        
        self.max_waypoints = max_waypoints
        self.feature_dim = feature_dim
        self.num_path_modes = num_path_modes
        
        # ì‹œê°„ ì„ë² ë”© (í™•ì‚° ê³¼ì • ì œì–´)
        self.time_embedding = SinusoidalPositionEmbedding(feature_dim)
        
        # ìœ„ì¹˜ ì„ë² ë”© (ì‹œì‘ì , ëª©í‘œì )
        self.pos_embedding = nn.Sequential(
            nn.Linear(4, feature_dim // 2),
            nn.ReLU(),
            nn.Linear(feature_dim // 2, feature_dim),
            nn.ReLU()
        )
        
        # ì›¨ì´í¬ì¸íŠ¸ ì„ë² ë”© (ë‹¤ì¤‘ ê²½ë¡œ í‘œí˜„)
        self.waypoint_embedding = nn.Sequential(
            nn.Linear(max_waypoints * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, feature_dim)
        )
        
        # ì‚¬íšŒì  ë§¥ë½ ëª¨ë‹¬ë¦¬í‹° ì„ë² ë”© (ë‹¤ì–‘í•œ ê²½ë¡œ ìŠ¤íƒ€ì¼)
        self.mode_embedding = nn.Embedding(num_path_modes, feature_dim)
        
        # ì¡°ê±´ë¶€ ìœµí•© ë ˆì´ì–´ë“¤ (ì‚¬íšŒì  ë§¥ë½ í†µí•©)
        self.fusion_layers = nn.ModuleList([
            SocialContextFusionLayer(feature_dim, hidden_dim) for _ in range(num_layers)
        ])
        
        # ë‹¤ì¤‘ ëª¨ë‹¬ ê²½ë¡œ ìƒì„± í—¤ë“œ
        self.multi_modal_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(feature_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, max_waypoints * 2)
            ) for _ in range(num_path_modes)
        ])
        
        # ê²½ë¡œ ëª¨ë“œ ì„ íƒê¸° (ìƒí™©ë³„ ìµœì  ê²½ë¡œ ìŠ¤íƒ€ì¼ ê²°ì •)
        self.mode_selector = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, num_path_modes),
            nn.Softmax(dim=-1)
        )
        
        # ì›¨ì´í¬ì¸íŠ¸ ìˆ˜ ì˜ˆì¸¡ (ì ì‘ì  ê²½ë¡œ ê¸¸ì´)
        self.num_waypoints_head = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, max_waypoints),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, 
                noisy_waypoints: torch.Tensor,
                timesteps: torch.Tensor,
                visual_features: torch.Tensor,
                start_pos: torch.Tensor,
                goal_pos: torch.Tensor,
                path_mode: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            noisy_waypoints: [B, max_waypoints, 2] ì¡ìŒì´ ì¶”ê°€ëœ ì›¨ì´í¬ì¸íŠ¸
            timesteps: [B] í™•ì‚° ì‹œê°„ ë‹¨ê³„
            visual_features: [B, feature_dim] í†µí•© ë¹„ìš©ë§µ ì‹œê° íŠ¹ì§•
            start_pos: [B, 2] ì‹œì‘ ìœ„ì¹˜
            goal_pos: [B, 2] ëª©í‘œ ìœ„ì¹˜
            path_mode: [B] ê²½ë¡œ ëª¨ë“œ (ì„ íƒì )
        
        Returns:
            predicted_noise: [B, max_waypoints, 2] ì˜ˆì¸¡ëœ ì¡ìŒ
            mode_probs: [B, num_path_modes] ê²½ë¡œ ëª¨ë“œ í™•ë¥ 
            num_waypoints_prob: [B, max_waypoints] ì›¨ì´í¬ì¸íŠ¸ ìˆ˜ í™•ë¥ 
        """
        batch_size = noisy_waypoints.shape[0]
        
        # ì‹œê°„ ì„ë² ë”© (í™•ì‚° ê³¼ì • ì œì–´)
        time_emb = self.time_embedding(timesteps)  # [B, feature_dim]
        
        # ìœ„ì¹˜ ì„ë² ë”© (ì‹œì‘/ëª©í‘œì  ë§¥ë½)
        start_goal = torch.cat([start_pos, goal_pos], dim=-1)  # [B, 4]
        pos_emb = self.pos_embedding(start_goal)  # [B, feature_dim]
        
        # ì›¨ì´í¬ì¸íŠ¸ ì„ë² ë”©
        waypoints_flat = noisy_waypoints.view(batch_size, -1)  # [B, max_waypoints*2]
        waypoint_emb = self.waypoint_embedding(waypoints_flat)  # [B, feature_dim]
        
        # ê²½ë¡œ ëª¨ë“œ ì„ë² ë”© (ë‹¤ì–‘í•œ ì‚¬íšŒì  ë§¥ë½ ìŠ¤íƒ€ì¼)
        if path_mode is None:
            # í•™ìŠµ ì‹œ: ëª¨ë“  ëª¨ë“œì˜ í‰ê·  ì‚¬ìš©
            mode_emb = self.mode_embedding.weight.mean(dim=0).unsqueeze(0).expand(batch_size, -1)
        else:
            mode_emb = self.mode_embedding(path_mode)
        
        # ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ íŠ¹ì§• ìœµí•©
        x = waypoint_emb + mode_emb  # ëª¨ë‹¬ë¦¬í‹° ì¡°ê±´ë¶€ ì„ë² ë”©
        for layer in self.fusion_layers:
            x = layer(x, visual_features, time_emb, pos_emb)
        
        # ê²½ë¡œ ëª¨ë“œ í™•ë¥  ì˜ˆì¸¡ (ìƒí™©ë³„ ìµœì  ìŠ¤íƒ€ì¼ ê²°ì •)
        mode_probs = self.mode_selector(x)  # [B, num_path_modes]
        
        # ë‹¤ì¤‘ ëª¨ë‹¬ ì¡ìŒ ì˜ˆì¸¡ (í™•ë¥ ì  ë‹¤ì¤‘ ê²½ë¡œ ìƒì„±)
        if path_mode is None:
            # í•™ìŠµ ì‹œ: ê°€ì¤‘ í‰ê·  ì‚¬ìš©
            multi_noise = []
            for head in self.multi_modal_heads:
                noise_pred = head(x)  # [B, max_waypoints*2]
                multi_noise.append(noise_pred)
            
            multi_noise = torch.stack(multi_noise, dim=1)  # [B, num_modes, max_waypoints*2]
            # ëª¨ë“œ í™•ë¥ ë¡œ ê°€ì¤‘ í‰ê· 
            predicted_noise = torch.sum(multi_noise * mode_probs.unsqueeze(-1), dim=1)
        else:
            # ì¶”ë¡  ì‹œ: íŠ¹ì • ëª¨ë“œ ì‚¬ìš©
            selected_heads = []
            for i, head in enumerate(self.multi_modal_heads):
                selected_heads.append(head(x))
            
            multi_noise = torch.stack(selected_heads, dim=1)  # [B, num_modes, max_waypoints*2]
            # ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ íŠ¹ì • ëª¨ë“œ ì„ íƒ
            mode_onehot = F.one_hot(path_mode, num_classes=self.num_path_modes).float()
            predicted_noise = torch.sum(multi_noise * mode_onehot.unsqueeze(-1), dim=1)
        
        predicted_noise = predicted_noise.view(batch_size, self.max_waypoints, 2)
        
        # ì ì‘ì  ì›¨ì´í¬ì¸íŠ¸ ìˆ˜ ì˜ˆì¸¡
        num_waypoints_prob = self.num_waypoints_head(x)  # [B, max_waypoints]
        
        return predicted_noise, mode_probs, num_waypoints_prob

class SocialContextFusionLayer(nn.Module):
    """ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ ìœµí•© ë ˆì´ì–´ - ë‹¤ì¤‘ ì¡°ê±´ë¶€ ì •ë³´ í†µí•©"""
    
    def __init__(self, feature_dim, hidden_dim):
        super().__init__()
        
        self.norm1 = nn.LayerNorm(feature_dim)
        self.norm2 = nn.LayerNorm(feature_dim)
        
        # ì…€í”„ ì–´í…ì…˜
        self.self_attn = nn.MultiheadAttention(feature_dim, num_heads=8, batch_first=True)
        
        # ì‚¬íšŒì  ë§¥ë½ ì¡°ê±´ë¶€ ë³€ì¡° (FiLM - Feature-wise Linear Modulation)
        self.visual_modulation = nn.Linear(feature_dim, feature_dim * 2)  # í†µí•© ë¹„ìš©ë§µ ë³€ì¡°
        self.time_modulation = nn.Linear(feature_dim, feature_dim * 2)    # í™•ì‚° ì‹œê°„ ë³€ì¡°
        self.pos_modulation = nn.Linear(feature_dim, feature_dim * 2)     # ìœ„ì¹˜ ë§¥ë½ ë³€ì¡°
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ (CCTV ê¸°ë°˜ ê¸€ë¡œë²Œ ë§¥ë½ ì¸ì‹)
        self.cross_attn = nn.MultiheadAttention(feature_dim, num_heads=4, batch_first=True)
        
        # í”¼ë“œí¬ì›Œë“œ
        self.ff = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, feature_dim)
        )
        
    def forward(self, x, visual_features, time_emb, pos_emb):
        # ì •ê·œí™”
        x_norm = self.norm1(x)
        
        # ì‚¬íšŒì  ë§¥ë½ ì¡°ê±´ë¶€ ë³€ì¡° (ë‹¤ì¤‘ ëª¨ë‹¬ ì •ë³´ í†µí•©)
        visual_scale, visual_shift = self.visual_modulation(visual_features).chunk(2, dim=-1)
        time_scale, time_shift = self.time_modulation(time_emb).chunk(2, dim=-1)
        pos_scale, pos_shift = self.pos_modulation(pos_emb).chunk(2, dim=-1)
        
        # ì¢…í•© ë³€ì¡° (ì •ì  + ë™ì  ì‚¬íšŒì  ë§¥ë½)
        scale = visual_scale + time_scale + pos_scale
        shift = visual_shift + time_shift + pos_shift
        x_modulated = x_norm * (1 + scale) + shift
        
        # ì…€í”„ ì–´í…ì…˜ (ê²½ë¡œ ë‚´ë¶€ ì¼ê´€ì„±)
        x_expanded = x_modulated.unsqueeze(1)  # [B, 1, feature_dim]
        self_attn_out, _ = self.self_attn(x_expanded, x_expanded, x_expanded)
        self_attn_out = self_attn_out.squeeze(1)  # [B, feature_dim]
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ (CCTV ê¸€ë¡œë²Œ ë§¥ë½ê³¼ì˜ ìƒí˜¸ì‘ìš©)
        visual_expanded = visual_features.unsqueeze(1)  # [B, 1, feature_dim]
        x_for_cross = x_modulated.unsqueeze(1)
        cross_attn_out, _ = self.cross_attn(x_for_cross, visual_expanded, visual_expanded)
        cross_attn_out = cross_attn_out.squeeze(1)  # [B, feature_dim]
        
        # ë‹¤ì¤‘ ì–´í…ì…˜ ìœµí•© (ë¡œì»¬ + ê¸€ë¡œë²Œ ë§¥ë½)
        attn_out = self_attn_out + 0.5 * cross_attn_out
        
        # ì”ì°¨ ì—°ê²°
        x = x + attn_out
        
        # í”¼ë“œí¬ì›Œë“œ (ì‚¬íšŒì  ë§¥ë½ ì •ì œ)
        x = x + self.ff(self.norm2(x))
        
        return x

class CDIFModel(nn.Module):
    """CDIF: CCTV-informed Diffusion í†µí•© ëª¨ë¸
    
    í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ë¥¼ í™œìš©í•œ ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ ê²½ë¡œ ìƒì„±
    - ì •ì  ì¥ì• ë¬¼ + ë™ì  ì‚¬íšŒì  ìš”ì†Œ í†µí•© ëª¨ë¸ë§
    - í™•ë¥ ì  ë‹¤ì¤‘ ê²½ë¡œ í›„ë³´ ë™ì‹œ ìƒì„±
    - ì‹¤ì‹œê°„ ì ì‘í˜• ë‚´ë¹„ê²Œì´ì…˜ ì§€ì›
    """
    
    def __init__(self, 
                 max_waypoints=8,
                 feature_dim=256,
                 hidden_dim=512,
                 num_layers=6,
                 num_path_modes=3):
        super().__init__()
        
        self.max_waypoints = max_waypoints
        self.num_path_modes = num_path_modes
        
        # í†µí•© ë¹„ìš©ë§µ ì¸ì½”ë” (í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ í™œìš©)
        self.cost_map_encoder = IntegratedCostMapEncoder(
            input_channels=3,  # ì •ì ë§µ + ì‚¬íšŒì ë¹„ìš© + íë¦„ë§µ
            feature_dim=feature_dim
        )
        
        # ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ í™•ì‚° ë„¤íŠ¸ì›Œí¬
        self.diffusion_net = SocialContextAwareDiffusionNet(
            max_waypoints=max_waypoints,
            feature_dim=feature_dim,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            num_path_modes=num_path_modes
        )
        
    def forward(self, 
                integrated_cost_map: torch.Tensor,
                noisy_waypoints: torch.Tensor,
                timesteps: torch.Tensor,
                start_pos: torch.Tensor,
                goal_pos: torch.Tensor,
                path_mode: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        CDIF ìˆœì „íŒŒ: ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ ë‹¤ì¤‘ ê²½ë¡œ ìƒì„±
        
        Args:
            integrated_cost_map: [B, 3, 60, 60] í†µí•© ë¹„ìš©ë§µ (ì •ì  + ì‚¬íšŒì  + íë¦„)
            noisy_waypoints: [B, max_waypoints, 2] ì¡ìŒì´ ì¶”ê°€ëœ ì›¨ì´í¬ì¸íŠ¸
            timesteps: [B] í™•ì‚° ì‹œê°„ ë‹¨ê³„
            start_pos: [B, 2] ì‹œì‘ ìœ„ì¹˜ (ì •ê·œí™”ë¨)
            goal_pos: [B, 2] ëª©í‘œ ìœ„ì¹˜ (ì •ê·œí™”ë¨)
            path_mode: [B] ê²½ë¡œ ëª¨ë“œ (0: ì§ì ‘, 1: ì‚¬íšŒì , 2: ìš°íšŒ)
        
        Returns:
            predicted_noise: [B, max_waypoints, 2] ì˜ˆì¸¡ëœ ì¡ìŒ
            mode_probs: [B, num_path_modes] ê²½ë¡œ ëª¨ë“œ í™•ë¥ 
            num_waypoints_prob: [B, max_waypoints] ì›¨ì´í¬ì¸íŠ¸ ìˆ˜ í™•ë¥ 
        """
        # í†µí•© ë¹„ìš©ë§µ íŠ¹ì§• ì¶”ì¶œ (í¸ì¬í•˜ëŠ” CCTV ì¸í”„ë¼ í™œìš©)
        visual_features = self.cost_map_encoder(integrated_cost_map)
        
        # ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ í™•ì‚° ê³¼ì •
        predicted_noise, mode_probs, num_waypoints_prob = self.diffusion_net(
            noisy_waypoints=noisy_waypoints,
            timesteps=timesteps,
            visual_features=visual_features,
            start_pos=start_pos,
            goal_pos=goal_pos,
            path_mode=path_mode
        )
        
        return predicted_noise, mode_probs, num_waypoints_prob

class DDPMScheduler:
    """DDPM ìŠ¤ì¼€ì¤„ëŸ¬ (GPU ìµœì í™”)"""
    
    def __init__(self, 
                 num_train_timesteps=1000,
                 beta_start=0.0001,
                 beta_end=0.02,
                 beta_schedule="linear"):
        self.num_train_timesteps = num_train_timesteps
        
        # ë² íƒ€ ìŠ¤ì¼€ì¤„
        if beta_schedule == "linear":
            self.betas = torch.linspace(beta_start, beta_end, num_train_timesteps)
        elif beta_schedule == "cosine":
            self.betas = self._cosine_beta_schedule(num_train_timesteps)
        
        # ì•ŒíŒŒ ê³„ì‚°
        self.alphas = 1.0 - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)
        
        # ìƒ˜í”Œë§ì„ ìœ„í•œ ê³„ìˆ˜ë“¤
        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
        
    def _cosine_beta_schedule(self, timesteps, s=0.008):
        """ì½”ì‚¬ì¸ ë² íƒ€ ìŠ¤ì¼€ì¤„"""
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)
    
    def add_noise(self, original_samples, noise, timesteps):
        """ì›ë³¸ ìƒ˜í”Œì— ì¡ìŒ ì¶”ê°€"""
        sqrt_alpha_prod = self.sqrt_alphas_cumprod[timesteps]
        sqrt_one_minus_alpha_prod = self.sqrt_one_minus_alphas_cumprod[timesteps]
        
        # ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ ì°¨ì› ì¡°ì •
        sqrt_alpha_prod = sqrt_alpha_prod.view(-1, 1, 1)
        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.view(-1, 1, 1)
        
        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
        return noisy_samples
    
    def to(self, device):
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        self.betas = self.betas.to(device)
        self.alphas = self.alphas.to(device)
        self.alphas_cumprod = self.alphas_cumprod.to(device)
        self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device)
        self.sqrt_alphas_cumprod = self.sqrt_alphas_cumprod.to(device)
        self.sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod.to(device)
        return self

if __name__ == "__main__":
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # CDIF ëª¨ë¸ ìƒì„± (ë‹¤ì¤‘ ê²½ë¡œ ëª¨ë“œ)
    model = CDIFModel(max_waypoints=8, num_path_modes=3).to(device)
    scheduler = DDPMScheduler().to(device)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (í†µí•© ë¹„ìš©ë§µ)
    batch_size = 4
    integrated_cost_map = torch.randn(batch_size, 3, 60, 60).to(device)  # 3ì±„ë„
    noisy_waypoints = torch.randn(batch_size, 8, 2).to(device)
    timesteps = torch.randint(0, 1000, (batch_size,)).to(device)
    start_pos = torch.randn(batch_size, 2).to(device)
    goal_pos = torch.randn(batch_size, 2).to(device)
    path_mode = torch.randint(0, 3, (batch_size,)).to(device)  # 0: ì§ì ‘, 1: ì‚¬íšŒì , 2: ìš°íšŒ
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ (ë‹¤ì¤‘ ëª¨ë‹¬ ì¶œë ¥)
    with torch.no_grad():
        predicted_noise, mode_probs, num_waypoints_prob = model(
            integrated_cost_map, noisy_waypoints, timesteps, start_pos, goal_pos, path_mode
        )
    
    print(f"ğŸ¯ CDIF ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"  - Predicted noise shape: {predicted_noise.shape}")
    print(f"  - Mode probabilities shape: {mode_probs.shape}")
    print(f"  - Num waypoints prob shape: {num_waypoints_prob.shape}")
    print(f"  - Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  - ê²½ë¡œ ëª¨ë“œë³„ í™•ë¥ : {mode_probs[0].cpu().numpy()}")
    print("âœ… CDIF: ì‚¬íšŒì  ë§¥ë½ ì¸ì‹ ë‹¤ì¤‘ ê²½ë¡œ ìƒì„± ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 