import numpy as np
import matplotlib.pyplot as plt
import xml.etree.ElementTree as ET
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import argparse
import os
from tqdm import tqdm
import json

# DiPPeR ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (robot_simuator_dippeR.pyì—ì„œ ê°€ì ¸ì˜´)
from robot_simuator_dippeR import Agent, ResNetEncoder, NoisePredictor, DiPPeR, RobotSimulatorDiPPeR

class SimulationDataset(Dataset):
    """ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ìƒì„±ëœ í•©ì„± ë°ì´í„°ì…‹"""
    def __init__(self, data_list):
        self.data = data_list
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ë°ì´í„° í˜•íƒœ: (cost_map, start_pos, goal_pos, path)
        cost_map = torch.from_numpy(item['cost_map']).float().unsqueeze(0)  # (1, 60, 60)
        start_pos = torch.from_numpy(item['start_pos']).float()  # (2,)
        goal_pos = torch.from_numpy(item['goal_pos']).float()  # (2,)
        path = torch.from_numpy(item['path']).float()  # (path_length, 2)
        
        return cost_map, start_pos, goal_pos, path

class SimulationDataCollector:
    """ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ í•©ì„± ë°ì´í„° ìˆ˜ì§‘"""
    def __init__(self, xml_file, visualize=False):
        self.simulator = RobotSimulatorDiPPeR(xml_file, model_path=None)
        self.visualize = visualize
        self.collected_data = []
        
        # A* í´ë°±ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì • (DiPPeR ë¹„í™œì„±í™”)
        self.simulator.use_dipperp = False
        
    def collect_data_episode(self, start_pos, goal_pos, max_steps=200):
        """í•œ ì—í”¼ì†Œë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ - ë‹¤ì–‘í•œ ê²½ë¡œ ìŠ¤íƒ€ì¼ í¬í•¨"""
        print(f"ë°ì´í„° ìˆ˜ì§‘: ({start_pos[0]:.2f}, {start_pos[1]:.2f}) â†’ ({goal_pos[0]:.2f}, {goal_pos[1]:.2f})")
        
        # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
        self.simulator.robot_pos = start_pos.copy()
        self.simulator.target_pos = goal_pos.copy()
        self.simulator.stuck_count = 0
        self.simulator.timestep_counter = 0
        
        # ì‹œê°í™” ì°½ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        if hasattr(self.simulator, 'fig') and self.simulator.fig is not None:
            plt.close(self.simulator.fig)
            self.simulator.fig = None
        
        episode_data = []
        stuck_counter = 0
        last_pos = None
        
        for step in range(max_steps):
            # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.simulator.update()
            
            # ì •ì²´ ìƒíƒœ ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
            current_pos = self.simulator.robot_pos.copy()
            current_pos_tuple = (round(current_pos[0], 1), round(current_pos[1], 1))  # 0.1m ì •ë°€ë„
            
            if last_pos == current_pos_tuple:
                stuck_counter += 1
                if stuck_counter > 20:  # 20ìŠ¤í… ë™ì•ˆ ê°™ì€ ìœ„ì¹˜ì— ìˆìœ¼ë©´ ì¤‘ë‹¨ (ë” ë¹ ë¥´ê²Œ)
                    print(f"ë¡œë´‡ ì •ì²´ ìƒíƒœ ê°ì§€. ì—í”¼ì†Œë“œ ì¤‘ë‹¨. ìŠ¤í…: {step}, ìˆ˜ì§‘ëœ ë°ì´í„°: {len(episode_data)}ê°œ")
                    break
            else:
                stuck_counter = 0
            last_pos = current_pos_tuple
            
            # ë‹¤ì–‘í•œ ê²½ë¡œ ìŠ¤íƒ€ì¼ ìƒì„±
            if self.simulator.target_pos is not None:
                paths_to_collect = []
                
                # 1. ê¸°ë³¸ A* ê²½ë¡œ (30% í™•ë¥  - ê°ì†Œ)
                if np.random.random() < 0.3:
                    astar_path = self.simulator.fallback_astar_planning(current_pos, goal_pos)
                    if astar_path and len(astar_path) > 2:
                        paths_to_collect.append(("astar", astar_path))
                
                # 2. ì‚¬íšŒì  ë¹„ìš© ê°•í™” A* ê²½ë¡œ (40% í™•ë¥  - ì¦ê°€)
                if np.random.random() < 0.4:
                    social_path = self.generate_social_aware_path(current_pos, goal_pos)
                    if social_path and len(social_path) > 2:
                        paths_to_collect.append(("social", social_path))
                
                # 3. ìš°íšŒ ê²½ë¡œ (30% í™•ë¥  - ì¦ê°€) - ì¤‘ê°„ì ì„ ê±°ì³ê°€ëŠ” ê²½ë¡œ
                if np.random.random() < 0.3:
                    detour_path = self.generate_detour_path(current_pos, goal_pos)
                    if detour_path and len(detour_path) > 2:
                        paths_to_collect.append(("detour", detour_path))
                
                # ìˆ˜ì§‘ëœ ê²½ë¡œë“¤ì„ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
                for path_type, path in paths_to_collect:
                    # ê²½ë¡œë¥¼ ê³ ì • ê¸¸ì´ë¡œ ë§ì¶¤ (50ê°œ)
                    path_length = 50
                    if len(path) >= path_length:
                        # ë‹¤ìš´ìƒ˜í”Œë§
                        indices = np.linspace(0, len(path)-1, path_length, dtype=int)
                        resampled_path = [path[i] for i in indices]
                    else:
                        # ì—…ìƒ˜í”Œë§ (ì„ í˜• ë³´ê°„)
                        resampled_path = self.interpolate_path(path, path_length)
                    
                    # ê²½ë¡œ ì•ˆì „ì„± ê²€ì¦ (ì¥ì• ë¬¼ ê´€í†µ ì²´í¬)
                    path_is_safe = True
                    for point in resampled_path:
                        if not self.simulator.is_position_safe(point):
                            path_is_safe = False
                            break
                    
                    # ì•ˆì „í•œ ê²½ë¡œë§Œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©
                    if path_is_safe:
                        # ë°ì´í„° ì €ì¥
                        data_item = {
                            'cost_map': self.simulator.fused_cost_map.copy(),
                            'start_pos': np.array([current_pos[0]/6.0, current_pos[1]/6.0]),  # ì •ê·œí™”
                            'goal_pos': np.array([goal_pos[0]/6.0, goal_pos[1]/6.0]),  # ì •ê·œí™”
                            'path': np.array([[p[0]/6.0, p[1]/6.0] for p in resampled_path]),  # ì •ê·œí™”
                            'path_type': path_type  # ê²½ë¡œ íƒ€ì… ì¶”ê°€
                        }
                        episode_data.append(data_item)
                        print(f"ê²½ë¡œ ìˆ˜ì§‘: {path_type} ({len(resampled_path)}ê°œ ì›¨ì´í¬ì¸íŠ¸)")
            
            # ì‹œê°í™”
            if self.visualize and step % 10 == 0:
                self.simulator.visualize()
                time.sleep(0.01)
            
            # ëª©í‘œ ë„ë‹¬ ì²´í¬
            dist_to_goal = np.sqrt((current_pos[0] - goal_pos[0])**2 + (current_pos[1] - goal_pos[1])**2)
            if dist_to_goal < 0.3:
                print(f"ëª©í‘œ ë„ë‹¬! ìŠ¤í…: {step}, ìˆ˜ì§‘ëœ ë°ì´í„°: {len(episode_data)}ê°œ")
                break
        
        self.collected_data.extend(episode_data)
        return len(episode_data)
    
    def interpolate_path(self, path, target_length):
        """ê²½ë¡œë¥¼ ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§"""
        if len(path) < 2:
            return path
        
        # ê²½ë¡œ ê¸¸ì´ ê³„ì‚°
        path_array = np.array(path)
        distances = np.cumsum([0] + [np.linalg.norm(path_array[i+1] - path_array[i]) 
                                     for i in range(len(path)-1)])
        total_distance = distances[-1]
        
        # ê· ë“± ê°„ê²©ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        target_distances = np.linspace(0, total_distance, target_length)
        resampled_path = []
        
        for target_dist in target_distances:
            # ê°€ì¥ ê°€ê¹Œìš´ êµ¬ê°„ ì°¾ê¸°
            idx = np.searchsorted(distances, target_dist)
            if idx == 0:
                resampled_path.append(path[0])
            elif idx >= len(path):
                resampled_path.append(path[-1])
            else:
                # ì„ í˜• ë³´ê°„
                t = (target_dist - distances[idx-1]) / (distances[idx] - distances[idx-1])
                interpolated = [(1-t) * path[idx-1][j] + t * path[idx][j] for j in range(2)]
                resampled_path.append(interpolated)
        
        return resampled_path
    
    def generate_social_aware_path(self, start_pos, goal_pos):
        """ì‚¬íšŒì  ë¹„ìš©ì„ ë” ê°•í•˜ê²Œ ê³ ë ¤í•œ ê²½ë¡œ ìƒì„±"""
        # ì„ì‹œë¡œ ì‚¬íšŒì  ë¹„ìš© ê°€ì¤‘ì¹˜ë¥¼ ë†’ì—¬ì„œ A* ì‹¤í–‰
        original_fused_map = self.simulator.fused_cost_map.copy()
        
        # ì—ì´ì „íŠ¸ ì£¼ë³€ ë¹„ìš©ì„ ë” ë†’ê²Œ ì„¤ì •
        for agent in self.simulator.agents:
            agent_pos = agent.pos
            x_idx = int((agent_pos[0] + 6) / self.simulator.grid_size)
            y_idx = int((agent_pos[1] + 6) / self.simulator.grid_size)
            
            # ì—ì´ì „íŠ¸ ì£¼ë³€ 3x3 ì˜ì—­ì— ë†’ì€ ë¹„ìš© ë¶€ì—¬
            for dx in range(-2, 3):
                for dy in range(-2, 3):
                    nx, ny = x_idx + dx, y_idx + dy
                    if 0 <= nx < 60 and 0 <= ny < 60:
                        distance = np.sqrt(dx*dx + dy*dy)
                        if distance <= 2:
                            social_penalty = 0.9 * (1 - distance/2)  # ê±°ë¦¬ì— ë”°ë¥¸ í˜ë„í‹°
                            self.simulator.fused_cost_map[ny, nx] = min(
                                self.simulator.fused_cost_map[ny, nx] + social_penalty, 0.95
                            )
        
        # ìˆ˜ì •ëœ ì½”ìŠ¤íŠ¸ ë§µìœ¼ë¡œ A* ì‹¤í–‰
        path = self.simulator.fallback_astar_planning(start_pos, goal_pos)
        
        # ì›ë˜ ì½”ìŠ¤íŠ¸ ë§µ ë³µì›
        self.simulator.fused_cost_map = original_fused_map
        
        return path
    
    def generate_detour_path(self, start_pos, goal_pos):
        """ì¤‘ê°„ì ì„ ê±°ì³ê°€ëŠ” ìš°íšŒ ê²½ë¡œ ìƒì„±"""
        # ì‹œì‘ì ê³¼ ëª©í‘œì  ì‚¬ì´ì˜ ì¤‘ì  ê·¼ì²˜ì— ëœë¤ ì¤‘ê°„ì  ìƒì„±
        mid_x = (start_pos[0] + goal_pos[0]) / 2
        mid_y = (start_pos[1] + goal_pos[1]) / 2
        
        # ì¤‘ì ì—ì„œ ëœë¤í•˜ê²Œ ì˜¤í”„ì…‹ ì¶”ê°€
        offset_range = 2.0
        for attempt in range(10):
            waypoint = [
                mid_x + np.random.uniform(-offset_range, offset_range),
                mid_y + np.random.uniform(-offset_range, offset_range)
            ]
            
            if self.simulator.is_position_safe(waypoint):
                # ì‹œì‘ì  â†’ ì¤‘ê°„ì  â†’ ëª©í‘œì  ê²½ë¡œ ìƒì„±
                path1 = self.simulator.fallback_astar_planning(start_pos, waypoint)
                path2 = self.simulator.fallback_astar_planning(waypoint, goal_pos)
                
                if path1 and path2 and len(path1) > 1 and len(path2) > 1:
                    # ë‘ ê²½ë¡œ ì—°ê²° (ì¤‘ë³µ ì œê±°)
                    combined_path = path1 + path2[1:]
                    return combined_path
        
        # ìš°íšŒ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ A* ê²½ë¡œ ë°˜í™˜
        return self.simulator.fallback_astar_planning(start_pos, goal_pos)
    
    def collect_random_episodes(self, num_episodes=100):
        """ëœë¤í•œ ì‹œì‘/ëª©í‘œì ìœ¼ë¡œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘"""
        total_data = 0
        
        for episode in tqdm(range(num_episodes), desc="ë°ì´í„° ìˆ˜ì§‘"):
            # ì•ˆì „í•œ ëœë¤ ìœ„ì¹˜ ìƒì„± í•¨ìˆ˜ (ë” ë„“ì€ ì•ˆì „ êµ¬ì—­ì—ì„œ ìƒì„±)
            def generate_safe_position(max_attempts=50):
                # ë¨¼ì € ì•ˆì „í•œ êµ¬ì—­ì—ì„œ ì‹œë„
                safe_zones = [
                    (-4.5, -0.5, -4.5, 1.5),  # ì™¼ìª½ ìœ„ êµ¬ì—­
                    (-4.5, -0.5, -2.0, -1.5), # ì™¼ìª½ ì•„ë˜ êµ¬ì—­  
                    (2.5, 3.5, -4.5, -0.5),   # ì˜¤ë¥¸ìª½ ì•„ë˜ êµ¬ì—­
                    (2.5, 3.5, 1.0, 2.0),     # ì˜¤ë¥¸ìª½ ìœ„ êµ¬ì—­
                    (-0.5, 1.5, 3.0, 4.5)     # ìœ„ìª½ ì¤‘ì•™ êµ¬ì—­
                ]
                
                for _ in range(max_attempts):
                    # ì•ˆì „ êµ¬ì—­ ì¤‘ í•˜ë‚˜ ì„ íƒ
                    if np.random.random() < 0.8:  # 80% í™•ë¥ ë¡œ ì•ˆì „ êµ¬ì—­ì—ì„œ ì„ íƒ
                        zone = safe_zones[np.random.randint(len(safe_zones))]
                        pos = [np.random.uniform(zone[0], zone[1]), 
                               np.random.uniform(zone[2], zone[3])]
                    else:  # 20% í™•ë¥ ë¡œ ì „ì²´ ì˜ì—­ì—ì„œ ì„ íƒ
                        pos = [np.random.uniform(-4.8, 4.8), np.random.uniform(-4.8, 4.8)]
                    
                    if self.simulator.is_position_safe(pos):
                        return pos
                
                # ìµœì•…ì˜ ê²½ìš° ê²€ì¦ëœ ì•ˆì „í•œ ìœ„ì¹˜ë“¤ ì¤‘ í•˜ë‚˜ ì„ íƒ
                safe_positions = [[-3.0, 0.0], [3.0, -2.0], [0.0, 3.5], [-2.5, -3.0], [3.0, 1.5]]
                for safe_pos in safe_positions:
                    if self.simulator.is_position_safe(safe_pos):
                        print(f"ì•ˆì „í•œ ê¸°ë³¸ ìœ„ì¹˜ ì‚¬ìš©: {safe_pos}")
                        return safe_pos
                
                print(f"ëª¨ë“  ì•ˆì „í•œ ìœ„ì¹˜ ìƒì„± ì‹¤íŒ¨, ì›ì  ì‚¬ìš©")
                return [0.0, 0.0]
            
            # ì—°ê²° ê°€ëŠ¥í•œ ì‹œì‘ì ê³¼ ëª©í‘œì  ìƒì„±
            max_pair_attempts = 10
            valid_pair_found = False
            
            for pair_attempt in range(max_pair_attempts):
                start_pos = generate_safe_position()
                goal_pos = generate_safe_position()
                
                # ì‹œì‘ì ê³¼ ëª©í‘œì ì´ ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ë‹¤ì‹œ ìƒì„±
                min_distance = 2.0
                distance_attempts = 0
                while (np.linalg.norm(np.array(start_pos) - np.array(goal_pos)) < min_distance and 
                       distance_attempts < 10):
                    goal_pos = generate_safe_position()
                    distance_attempts += 1
                
                # A* ê²½ë¡œ ê³„íšìœ¼ë¡œ ì—°ê²°ì„± ì²´í¬
                test_path = self.simulator.fallback_astar_planning(start_pos, goal_pos)
                
                if test_path and len(test_path) > 1:
                    valid_pair_found = True
                    print(f"ìœ íš¨í•œ ê²½ë¡œ ë°œê²¬: {start_pos} â†’ {goal_pos} ({len(test_path)}ê°œ ì›¨ì´í¬ì¸íŠ¸)")
                    break
                else:
                    print(f"ê²½ë¡œ ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {pair_attempt+1}/{max_pair_attempts}): {start_pos} â†’ {goal_pos}")
            
            if not valid_pair_found:
                print(f"ì—í”¼ì†Œë“œ {episode}: ì—°ê²° ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ê±´ë„ˆë›°ê¸°.")
                continue
            
            # ë°ì´í„° ìˆ˜ì§‘
            collected = self.collect_data_episode(start_pos, goal_pos)
            total_data += collected
            
            if episode % 10 == 0:
                print(f"ì—í”¼ì†Œë“œ {episode}/{num_episodes}, ì´ ë°ì´í„°: {total_data}ê°œ")
        
        print(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ì´ {total_data}ê°œ ë°ì´í„°")
        return self.collected_data

class DiPPeRTrainer:
    """DiPPeR ëª¨ë¸ í•™ìŠµ"""
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # GPU ì‚¬ìš© ì‹œ í•™ìŠµë¥  ì¡°ì • (ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ í•™ìŠµë¥  ëŒ€í­ ê°ì†Œ)
        if device.type == 'cuda':
            lr = 1e-5  # GPUì—ì„œë„ ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ
            print(f"ğŸš€ GPU í•™ìŠµë¥  (ê°œì„ ): {lr}")
        else:
            lr = 5e-6  # CPUì—ì„œëŠ” ë” ë‚®ì€ í•™ìŠµë¥ 
            print(f"ğŸ’» CPU í•™ìŠµë¥  (ê°œì„ ): {lr}")
            
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-6)
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=50, T_mult=2)
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.patience = 50  # Early stopping patience ëŒ€í­ ì¦ê°€ (ë” ê¸´ í•™ìŠµ)
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        if device.type == 'cuda':
            torch.backends.cudnn.benchmark = True  # ì„±ëŠ¥ ìµœì í™”
            torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìš°ì„ 
            print("CUDNN ìµœì í™” í™œì„±í™”")
        
        self.first_batch = False  # ì²« ë²ˆì§¸ ë°°ì¹˜ ì²´í¬ìš©
        
    def train_step(self, batch):
        cost_maps, start_pos, goal_pos, paths = batch
        batch_size = cost_maps.shape[0]
        
        # GPUë¡œ ì´ë™ (non_blocking=Trueë¡œ ì„±ëŠ¥ í–¥ìƒ)
        cost_maps = cost_maps.to(self.device, non_blocking=True)
        start_pos = start_pos.to(self.device, non_blocking=True)
        goal_pos = goal_pos.to(self.device, non_blocking=True)
        paths = paths.to(self.device, non_blocking=True)
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
        if hasattr(self, 'first_batch') and not self.first_batch:
            if self.device.type == 'cuda':
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
            self.first_batch = True
        
        # ëœë¤ íƒ€ì„ìŠ¤í… ì„ íƒ (ë” ë‹¤ì–‘í•œ ë²”ìœ„)
        timesteps = torch.randint(0, self.model.max_timesteps, (batch_size,), device=self.device)
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = torch.randn_like(paths)
        alpha_cumprod = self.model.alphas_cumprod[timesteps].view(-1, 1, 1)
        noisy_paths = torch.sqrt(alpha_cumprod) * paths + torch.sqrt(1 - alpha_cumprod) * noise
        
        # ì‹œì‘ì ê³¼ ëª©í‘œì  ì¡°ê±´
        start_goal_pos = torch.cat([start_pos, goal_pos], dim=-1)
        
        # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
        predicted_noise = self.model(cost_maps, noisy_paths, timesteps, start_goal_pos)
        
        # ì†ì‹¤ ê³„ì‚° (MSE Lossë¡œ ë³€ê²½ - ë” ì •í™•í•œ í•™ìŠµ)
        loss = nn.MSELoss()(predicted_noise, noise)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°•í™”
        self.optimizer.step()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 10ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤)
        if self.device.type == 'cuda' and hasattr(self, 'batch_count'):
            self.batch_count += 1
            if self.batch_count % 10 == 0:
                torch.cuda.empty_cache()
        elif self.device.type == 'cuda':
            self.batch_count = 1
        
        return loss.item()
    
    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        pbar = tqdm(dataloader, desc="Training")
        for batch in pbar:
            loss = self.train_step(batch)
            total_loss += loss
            num_batches += 1
            
            # ì‹¤ì‹œê°„ ì†ì‹¤ í‘œì‹œ
            current_avg_loss = total_loss / num_batches
            pbar.set_postfix({'Loss': f'{current_avg_loss:.6f}', 'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'})
        
        avg_loss = total_loss / num_batches
        self.scheduler.step()
        
        # Early stopping ì²´í¬ (ë” ì—„ê²©í•˜ê²Œ)
        if avg_loss < self.best_loss * 0.999:  # 0.1% ì´ìƒ ê°œì„ ë˜ì–´ì•¼ í•¨ (ë” ì—„ê²©)
            self.best_loss = avg_loss
            self.patience_counter = 0
            return avg_loss, True  # ê°œì„ ë¨
        else:
            self.patience_counter += 1
            return avg_loss, False  # ê°œì„  ì•ˆë¨
    
    def save_model(self, path):
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict()
        }, path)
        print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_model(self, path):
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")

def main():
    parser = argparse.ArgumentParser(description='DiPPeR ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--xml_file', default='scenarios/Congestion1.xml', help='ì‹œë®¬ë ˆì´ì…˜ XML íŒŒì¼')
    parser.add_argument('--num_episodes', type=int, default=1000, help='ë°ì´í„° ìˆ˜ì§‘ ì—í”¼ì†Œë“œ ìˆ˜ (ëŒ€í­ ì¦ê°€)')
    parser.add_argument('--epochs', type=int, default=500, help='í•™ìŠµ ì—í¬í¬ ìˆ˜ (ëŒ€í­ ì¦ê°€)')
    parser.add_argument('--batch_size', type=int, default=8, help='ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)')
    parser.add_argument('--visualize', action='store_true', help='ë°ì´í„° ìˆ˜ì§‘ ì‹œ ì‹œê°í™”')
    parser.add_argument('--save_data', help='ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--load_data', help='ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ê²½ë¡œ')
    parser.add_argument('--model_save_path', default='models/dipperp_model.pth', help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU, ì•„ë‹ˆë©´ CPU
    if torch.cuda.is_available():
        device = torch.device('cuda:0')  # ëª…ì‹œì ìœ¼ë¡œ GPU 0ë²ˆ ì§€ì •
        torch.cuda.set_device(0)  # GPU 0ë²ˆìœ¼ë¡œ ì„¤ì •
        print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"í˜„ì¬ GPU ë””ë°”ì´ìŠ¤: {torch.cuda.current_device()}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        print(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    else:
        device = torch.device('cpu')
        print("ğŸ’» GPU ì—†ìŒ, CPUë¡œ í•™ìŠµ")
    
    # ë°ì´í„° ìˆ˜ì§‘ ë˜ëŠ” ë¡œë“œ
    if args.load_data and os.path.exists(args.load_data):
        print(f"ë°ì´í„° ë¡œë“œ: {args.load_data}")
        with open(args.load_data, 'r') as f:
            data_list = json.load(f)
        # JSONì—ì„œ numpy ë°°ì—´ë¡œ ë³€í™˜
        for item in data_list:
            item['cost_map'] = np.array(item['cost_map'])
            item['start_pos'] = np.array(item['start_pos'])
            item['goal_pos'] = np.array(item['goal_pos'])
            item['path'] = np.array(item['path'])
    else:
        print("ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        collector = SimulationDataCollector(args.xml_file, visualize=args.visualize)
        data_list = collector.collect_random_episodes(args.num_episodes)
        
        # ë°ì´í„° ì €ì¥
        if args.save_data:
            # numpy ë°°ì—´ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
            json_data = []
            for item in data_list:
                json_item = {
                    'cost_map': item['cost_map'].tolist(),
                    'start_pos': item['start_pos'].tolist(),
                    'goal_pos': item['goal_pos'].tolist(),
                    'path': item['path'].tolist()
                }
                json_data.append(json_item)
            
            with open(args.save_data, 'w') as f:
                json.dump(json_data, f)
            print(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {args.save_data}")
    
    print(f"ì´ ë°ì´í„° ê°œìˆ˜: {len(data_list)}")
    
    # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    dataset = SimulationDataset(data_list)
    
    # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ num_workers ìë™ ì¡°ì •
    import multiprocessing
    num_workers = min(4, multiprocessing.cpu_count())
    
    # GPU ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
    if device.type == 'cuda':
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_memory_gb >= 8:
            recommended_batch_size = 16
        elif gpu_memory_gb >= 4:
            recommended_batch_size = 12
        else:
            recommended_batch_size = 8
        
        if args.batch_size == 8:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°ë§Œ ì¡°ì •
            args.batch_size = recommended_batch_size
            print(f"ğŸš€ GPU ë©”ëª¨ë¦¬ {gpu_memory_gb:.1f}GB ê°ì§€: ë°°ì¹˜ í¬ê¸°ë¥¼ {args.batch_size}ë¡œ ìë™ ì¡°ì •")
    
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True if device.type == 'cuda' else False,
        persistent_workers=True if num_workers > 0 else False
    )
    
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {args.batch_size}, ì›Œì»¤ ìˆ˜: {num_workers}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = DiPPeR(visual_feature_dim=512, path_dim=2, max_timesteps=1000)
    print(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™
    model = model.to(device)
    print(f"ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì™„ë£Œ")
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    if device.type == 'cuda':
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
    
    trainer = DiPPeRTrainer(model, device)
    
    # í•™ìŠµ
    print("í•™ìŠµ ì‹œì‘...")
    best_model_path = f"models/{args.model_save_path.split('/')[-1].split('.')[0]}_best.pth"
    
    for epoch in range(args.epochs):
        avg_loss, improved = trainer.train_epoch(dataloader)
        print(f"Epoch {epoch+1}/{args.epochs}, Loss: {avg_loss:.6f}, Best: {trainer.best_loss:.6f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if improved:
            trainer.save_model(best_model_path)
            print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_model_path}")
        
        # ì •ê¸° ëª¨ë¸ ì €ì¥ (10 ì—í¬í¬ë§ˆë‹¤)
        if (epoch + 1) % 10 == 0:
            save_path = f"models/{args.model_save_path.split('/')[-1].split('.')[0]}_epoch_{epoch+1}.pth"
            trainer.save_model(save_path)
        
        # Early stopping ì²´í¬
        if trainer.patience_counter >= trainer.patience:
            print(f"â¹ï¸ Early stopping: {trainer.patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    trainer.save_model(args.model_save_path)
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_path} (Loss: {trainer.best_loss:.6f})")

if __name__ == "__main__":
    main() 