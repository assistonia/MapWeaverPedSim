#!/usr/bin/env python3
"""
GPU ì„œë²„ìš© ìµœì í™”ëœ DiPPeR í•™ìŠµ ì½”ë“œ
- ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬
- ë©€í‹° GPU ì§€ì›
- ê³ ì„±ëŠ¥ ë°ì´í„°ë¡œë”
- ë©”ëª¨ë¦¬ ìµœì í™”
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib
matplotlib.use('Agg')  # ì„œë²„ìš© (GUI ì—†ìŒ)
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
from pathlib import Path
import random
import gc
import time
from torch.nn.parallel import DataParallel

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from robot_simuator_dippeR import DiPPeR, RobotSimulatorDiPPeR

class GPUOptimizedDataset(Dataset):
    """GPU ìµœì í™”ëœ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_dir='training_data_gpu'):
        self.data_dir = Path(data_dir)
        self.samples = []
        self._load_data()
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ í…ì„œ ë³€í™˜
        print("ğŸš€ GPU ìµœì í™”ë¥¼ ìœ„í•œ í…ì„œ ì‚¬ì „ ë³€í™˜...")
        self._preprocess_for_gpu()
    
    def _load_data(self):
        """ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë¡œë“œ"""
        data_files = list(self.data_dir.glob('*.json'))
        
        for file_path in tqdm(data_files, desc="ë°ì´í„° ë¡œë”©"):
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        self.samples.extend(data)
                    else:
                        self.samples.append(data)
            except Exception as e:
                print(f"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {file_path}: {e}")
        
        print(f"ì´ {len(self.samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
    
    def _preprocess_for_gpu(self):
        """GPU ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‚¬ì „ ë³€í™˜"""
        processed_samples = []
        
        for sample in tqdm(self.samples, desc="GPU ìµœì í™”"):
            try:
                # í…ì„œ ë³€í™˜ ë° ì •ê·œí™”
                cost_map = torch.from_numpy(np.array(sample['cost_map'])).float()
                start_pos = torch.from_numpy(np.array(sample['start_pos'])).float()
                goal_pos = torch.from_numpy(np.array(sample['goal_pos'])).float()
                path = torch.from_numpy(np.array(sample['path'])).float()
                
                processed_samples.append({
                    'cost_map': cost_map,
                    'start_pos': start_pos,
                    'goal_pos': goal_pos,
                    'path': path,
                    'path_type': sample.get('path_type', 'unknown')
                })
            except Exception as e:
                print(f"ìƒ˜í”Œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        self.samples = processed_samples
        print(f"GPU ìµœì í™” ì™„ë£Œ: {len(self.samples)}ê°œ ìƒ˜í”Œ")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ì´ë¯¸ í…ì„œë¡œ ë³€í™˜ëœ ë°ì´í„° ë°˜í™˜
        cost_map = sample['cost_map'].unsqueeze(0)  # (1, 60, 60)
        start_pos = sample['start_pos']  # (2,)
        goal_pos = sample['goal_pos']  # (2,)
        path = sample['path']  # (path_length, 2)
        
        return cost_map, start_pos, goal_pos, path

class GPUDataCollector:
    """GPU ì„œë²„ìš© ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, xml_file, output_dir='training_data_gpu'):
        self.simulator = RobotSimulatorDiPPeR(xml_file, model_path=None)
        self.simulator.use_dipperp = False  # A* ì‚¬ìš©
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    def collect_massive_data(self, target_samples=50000):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (GPU ì„œë²„ìš©)"""
        print(f"ğŸ¯ ëª©í‘œ: {target_samples:,}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘")
        
        collected_data = []
        batch_size = 1000  # 1000ê°œì”© ë°°ì¹˜ë¡œ ì €ì¥
        batch_count = 0
        
        pbar = tqdm(total=target_samples, desc="ë°ì´í„° ìˆ˜ì§‘")
        
        while len(collected_data) < target_samples:
            # ëœë¤ ì‹œì‘/ëª©í‘œì  ìƒì„±
            start_pos = self._generate_safe_position()
            goal_pos = self._generate_safe_position()
            
            if start_pos is None or goal_pos is None:
                continue
            
            # ê±°ë¦¬ ì²´í¬ (ë„ˆë¬´ ê°€ê¹ì§€ ì•Šê²Œ)
            dist = np.linalg.norm(np.array(start_pos) - np.array(goal_pos))
            if dist < 2.0:
                continue
            
            # ë‹¤ì–‘í•œ ê²½ë¡œ íƒ€ì… ìƒì„±
            episode_data = self._collect_episode_data(start_pos, goal_pos)
            collected_data.extend(episode_data)
            
            pbar.update(len(episode_data))
            
            # ë°°ì¹˜ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if len(collected_data) >= batch_size:
                self._save_batch(collected_data[:batch_size], batch_count)
                collected_data = collected_data[batch_size:]
                batch_count += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
        
        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if collected_data:
            self._save_batch(collected_data, batch_count)
        
        pbar.close()
        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {batch_count + 1}ê°œ ë°°ì¹˜ íŒŒì¼")
    
    def _generate_safe_position(self):
        """ì•ˆì „í•œ ìœ„ì¹˜ ìƒì„±"""
        # ì•ˆì „ êµ¬ì—­ ì •ì˜
        safe_zones = [
            (-5.5, -5.5, -0.5, -0.5),  # ì¢Œí•˜ë‹¨
            (-5.5, 0.5, -0.5, 5.5),    # ì¢Œìƒë‹¨
            (0.5, -5.5, 5.5, -0.5),    # ìš°í•˜ë‹¨
            (0.5, 0.5, 5.5, 5.5),      # ìš°ìƒë‹¨
            (-2.0, -2.0, 2.0, 2.0)     # ì¤‘ì•™
        ]
        
        for _ in range(50):
            zone = random.choice(safe_zones)
            x = random.uniform(zone[0], zone[2])
            y = random.uniform(zone[1], zone[3])
            
            if self.simulator.is_position_safe([x, y]):
                return [x, y]
        
        return None
    
    def _collect_episode_data(self, start_pos, goal_pos):
        """í•œ ì—í”¼ì†Œë“œì—ì„œ ë‹¤ì–‘í•œ ê²½ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
        episode_data = []
        
        # 1. ê¸°ë³¸ A* ê²½ë¡œ (40%)
        if random.random() < 0.4:
            path = self.simulator.fallback_astar_planning(start_pos, goal_pos)
            if path and len(path) > 5:
                data = self._create_data_sample(start_pos, goal_pos, path, "astar")
                if data:
                    episode_data.append(data)
        
        # 2. ì‚¬íšŒì  ë¹„ìš© ê°•í™” ê²½ë¡œ (35%)
        if random.random() < 0.35:
            path = self._generate_social_aware_path(start_pos, goal_pos)
            if path and len(path) > 5:
                data = self._create_data_sample(start_pos, goal_pos, path, "social")
                if data:
                    episode_data.append(data)
        
        # 3. ìš°íšŒ ê²½ë¡œ (25%)
        if random.random() < 0.25:
            path = self._generate_detour_path(start_pos, goal_pos)
            if path and len(path) > 5:
                data = self._create_data_sample(start_pos, goal_pos, path, "detour")
                if data:
                    episode_data.append(data)
        
        return episode_data
    
    def _create_data_sample(self, start_pos, goal_pos, path, path_type):
        """ë°ì´í„° ìƒ˜í”Œ ìƒì„±"""
        try:
            # ê²½ë¡œë¥¼ 50ê°œë¡œ ë¦¬ìƒ˜í”Œë§
            if len(path) >= 50:
                indices = np.linspace(0, len(path)-1, 50, dtype=int)
                resampled_path = [path[i] for i in indices]
            else:
                resampled_path = self._interpolate_path(path, 50)
            
            # ì•ˆì „ì„± ê²€ì¦
            for point in resampled_path:
                if not self.simulator.is_position_safe(point):
                    return None
            
            # ë°ì´í„° ìƒ˜í”Œ ìƒì„±
            return {
                'cost_map': self.simulator.fused_cost_map.copy(),
                'start_pos': np.array([start_pos[0]/6.0, start_pos[1]/6.0]),  # ì •ê·œí™”
                'goal_pos': np.array([goal_pos[0]/6.0, goal_pos[1]/6.0]),    # ì •ê·œí™”
                'path': np.array([[p[0]/6.0, p[1]/6.0] for p in resampled_path]),  # ì •ê·œí™”
                'path_type': path_type
            }
        except Exception as e:
            return None
    
    def _interpolate_path(self, path, target_length):
        """ê²½ë¡œ ì„ í˜• ë³´ê°„"""
        if len(path) < 2:
            return path
        
        path_array = np.array(path)
        distances = np.cumsum([0] + [np.linalg.norm(path_array[i+1] - path_array[i]) 
                                     for i in range(len(path)-1)])
        total_distance = distances[-1]
        
        target_distances = np.linspace(0, total_distance, target_length)
        resampled_path = []
        
        for target_dist in target_distances:
            idx = np.searchsorted(distances, target_dist)
            if idx == 0:
                resampled_path.append(path[0])
            elif idx >= len(path):
                resampled_path.append(path[-1])
            else:
                t = (target_dist - distances[idx-1]) / (distances[idx] - distances[idx-1])
                interpolated = [(1-t) * path[idx-1][j] + t * path[idx][j] for j in range(2)]
                resampled_path.append(interpolated)
        
        return resampled_path
    
    def _generate_social_aware_path(self, start_pos, goal_pos):
        """ì‚¬íšŒì  ë¹„ìš© ê°•í™” ê²½ë¡œ"""
        original_map = self.simulator.fused_cost_map.copy()
        
        # ì—ì´ì „íŠ¸ ì£¼ë³€ ë¹„ìš© ê°•í™”
        for agent in self.simulator.agents:
            x_idx = int((agent.pos[0] + 6) / self.simulator.grid_size)
            y_idx = int((agent.pos[1] + 6) / self.simulator.grid_size)
            
            for dx in range(-3, 4):
                for dy in range(-3, 4):
                    nx, ny = x_idx + dx, y_idx + dy
                    if 0 <= nx < 60 and 0 <= ny < 60:
                        distance = np.sqrt(dx*dx + dy*dy)
                        if distance <= 3:
                            penalty = 0.8 * (1 - distance/3)
                            self.simulator.fused_cost_map[ny, nx] = min(
                                self.simulator.fused_cost_map[ny, nx] + penalty, 0.95
                            )
        
        path = self.simulator.fallback_astar_planning(start_pos, goal_pos)
        self.simulator.fused_cost_map = original_map
        return path
    
    def _generate_detour_path(self, start_pos, goal_pos):
        """ìš°íšŒ ê²½ë¡œ ìƒì„±"""
        mid_x = (start_pos[0] + goal_pos[0]) / 2
        mid_y = (start_pos[1] + goal_pos[1]) / 2
        
        for _ in range(10):
            waypoint = [
                mid_x + random.uniform(-3.0, 3.0),
                mid_y + random.uniform(-3.0, 3.0)
            ]
            
            if self.simulator.is_position_safe(waypoint):
                path1 = self.simulator.fallback_astar_planning(start_pos, waypoint)
                path2 = self.simulator.fallback_astar_planning(waypoint, goal_pos)
                
                if path1 and path2 and len(path1) > 1 and len(path2) > 1:
                    return path1 + path2[1:]
        
        return self.simulator.fallback_astar_planning(start_pos, goal_pos)
    
    def _save_batch(self, data, batch_id):
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        filename = self.output_dir / f"batch_{batch_id:04d}.json"
        with open(filename, 'w') as f:
            json.dump(data, f, cls=NumpyEncoder)
        print(f"ë°°ì¹˜ ì €ì¥: {filename} ({len(data)}ê°œ ìƒ˜í”Œ)")

class NumpyEncoder(json.JSONEncoder):
    """NumPy ë°°ì—´ì„ JSONìœ¼ë¡œ ì¸ì½”ë”©"""
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

class GPUDiPPeRTrainer:
    """GPU ìµœì í™”ëœ DiPPeR í•™ìŠµê¸°"""
    
    def __init__(self, model, device='cuda'):
        self.device = torch.device(device)
        
        # ë©€í‹° GPU ì§€ì›
        if torch.cuda.device_count() > 1:
            print(f"ğŸš€ {torch.cuda.device_count()}ê°œ GPU ê°ì§€ - DataParallel ì‚¬ìš©")
            self.model = DataParallel(model)
        else:
            self.model = model
        
        self.model = self.model.to(self.device)
        
        # GPU ìµœì í™”ëœ ì„¤ì •
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=1e-4,  # GPUì—ì„œ ë” ë†’ì€ í•™ìŠµë¥ 
            weight_decay=1e-6,
            eps=1e-8
        )
        
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=5e-4,
            epochs=200,
            steps_per_epoch=100,  # ì¶”ì •ê°’
            pct_start=0.1
        )
        
        # í˜¼í•© ì •ë°€ë„ í•™ìŠµ (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
        self.scaler = torch.cuda.amp.GradScaler()
        
        self.best_loss = float('inf')
        self.patience_counter = 0
        self.patience = 20
        
        # GPU ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
    def train_step(self, batch):
        """GPU ìµœì í™”ëœ í•™ìŠµ ìŠ¤í…"""
        cost_maps, start_pos, goal_pos, paths = batch
        batch_size = cost_maps.shape[0]
        
        # GPUë¡œ ì´ë™ (non_blocking=Trueë¡œ ì„±ëŠ¥ í–¥ìƒ)
        cost_maps = cost_maps.to(self.device, non_blocking=True)
        start_pos = start_pos.to(self.device, non_blocking=True)
        goal_pos = goal_pos.to(self.device, non_blocking=True)
        paths = paths.to(self.device, non_blocking=True)
        
        # í˜¼í•© ì •ë°€ë„ í•™ìŠµ
        with torch.cuda.amp.autocast():
            # ëœë¤ íƒ€ì„ìŠ¤í…
            timesteps = torch.randint(0, 1000, (batch_size,), device=self.device)
            
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = torch.randn_like(paths)
            
            # DiPPeR ëª¨ë¸ì—ì„œ alphas_cumprod ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.model, 'module'):
                alphas_cumprod = self.model.module.alphas_cumprod
            else:
                alphas_cumprod = self.model.alphas_cumprod
            
            alpha_cumprod = alphas_cumprod[timesteps].view(-1, 1, 1)
            noisy_paths = torch.sqrt(alpha_cumprod) * paths + torch.sqrt(1 - alpha_cumprod) * noise
            
            # ì¡°ê±´ ì…ë ¥
            start_goal_pos = torch.cat([start_pos, goal_pos], dim=-1)
            
            # ë…¸ì´ì¦ˆ ì˜ˆì¸¡
            predicted_noise = self.model(cost_maps, noisy_paths, timesteps, start_goal_pos)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = nn.MSELoss()(predicted_noise, noise)
        
        # ì—­ì „íŒŒ (í˜¼í•© ì •ë°€ë„)
        self.optimizer.zero_grad()
        self.scaler.scale(loss).backward()
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item()
    
    def train_epoch(self, dataloader):
        """ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        
        pbar = tqdm(dataloader, desc="í•™ìŠµ ì¤‘")
        for batch in pbar:
            loss = self.train_step(batch)
            total_loss += loss
            
            pbar.set_postfix({'Loss': f'{loss:.6f}'})
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
        
        return total_loss / len(dataloader)
    
    def save_model(self, path, epoch=None, loss=None):
        """ëª¨ë¸ ì €ì¥"""
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        
        checkpoint = {
            'model_state_dict': model_to_save.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': epoch,
            'loss': loss
        }
        
        torch.save(checkpoint, path)
        print(f"ëª¨ë¸ ì €ì¥: {path}")

def main():
    """GPU ì„œë²„ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("=== GPU ìµœì í™” DiPPeR í•™ìŠµ ===")
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    device = torch.device('cuda')
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name()}")
    print(f"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # 1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
    print("\n1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ìˆ˜ì§‘...")
    collector = GPUDataCollector('scenarios/Circulation1.xml')
    collector.collect_massive_data(target_samples=50000)
    
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    print("\n2. GPU ìµœì í™” ë°ì´í„°ì…‹ ë¡œë“œ...")
    dataset = GPUOptimizedDataset('training_data_gpu')
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # ê³ ì„±ëŠ¥ ë°ì´í„°ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=128,  # GPUìš© ëŒ€ìš©ëŸ‰ ë°°ì¹˜
        shuffle=True,
        num_workers=8,
        pin_memory=True,
        persistent_workers=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=128,
        shuffle=False,
        num_workers=8,
        pin_memory=True,
        persistent_workers=True
    )
    
    print(f"í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ")
    
    # 3. ëª¨ë¸ ë° í•™ìŠµê¸° ìƒì„±
    print("\n3. GPU ìµœì í™” ëª¨ë¸ ìƒì„±...")
    model = DiPPeR(visual_feature_dim=512, path_dim=2, max_timesteps=1000)
    trainer = GPUDiPPeRTrainer(model, device='cuda')
    
    # 4. í•™ìŠµ ì‹œì‘
    print("\n4. GPU ê³ ì† í•™ìŠµ ì‹œì‘...")
    os.makedirs('models', exist_ok=True)
    
    for epoch in range(200):
        # í•™ìŠµ
        train_loss = trainer.train_epoch(train_loader)
        
        # ê²€ì¦ (ê°„ë‹¨íˆ)
        trainer.model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                cost_maps, start_pos, goal_pos, paths = batch
                cost_maps = cost_maps.to(device, non_blocking=True)
                start_pos = start_pos.to(device, non_blocking=True)
                goal_pos = goal_pos.to(device, non_blocking=True)
                paths = paths.to(device, non_blocking=True)
                
                batch_size = cost_maps.shape[0]
                timesteps = torch.randint(0, 1000, (batch_size,), device=device)
                noise = torch.randn_like(paths)
                
                if hasattr(trainer.model, 'module'):
                    alphas_cumprod = trainer.model.module.alphas_cumprod
                else:
                    alphas_cumprod = trainer.model.alphas_cumprod
                
                alpha_cumprod = alphas_cumprod[timesteps].view(-1, 1, 1)
                noisy_paths = torch.sqrt(alpha_cumprod) * paths + torch.sqrt(1 - alpha_cumprod) * noise
                start_goal_pos = torch.cat([start_pos, goal_pos], dim=-1)
                
                predicted_noise = trainer.model(cost_maps, noisy_paths, timesteps, start_goal_pos)
                loss = nn.MSELoss()(predicted_noise, noise)
                val_loss += loss.item()
        
        val_loss /= len(val_loader)
        
        print(f"Epoch {epoch+1}/200: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_loss < trainer.best_loss:
            trainer.best_loss = val_loss
            trainer.patience_counter = 0
            trainer.save_model(f'models/dipperp_gpu_best.pth', epoch, val_loss)
            print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: {val_loss:.6f}")
        else:
            trainer.patience_counter += 1
        
        # Early stopping
        if trainer.patience_counter >= trainer.patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
        
        # ì£¼ê¸°ì  ì €ì¥
        if (epoch + 1) % 10 == 0:
            trainer.save_model(f'models/dipperp_gpu_epoch_{epoch+1}.pth', epoch, val_loss)
    
    print("âœ… GPU í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 