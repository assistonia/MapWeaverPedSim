#!/usr/bin/env python3
"""
CDIF (CCTV-Diffusion) Training Pipeline
- GPU A6000 ì„œë²„ ìµœì í™”
- ì‚¬íšŒì  ë¹„ìš©ë§µ ê¸°ë°˜ ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ í•™ìŠµ
- ì‹¤ì‹œê°„ í•™ìŠµ ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦
"""

import os
import sys
import time
import json
import random
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import matplotlib
matplotlib.use('Agg')  # ì„œë²„ìš© ë°±ì—”ë“œ
import matplotlib.pyplot as plt
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from cdif_model import CDIFModel, DDPMScheduler
from robot_simulator_cgip import RobotSimulator

@dataclass
class CDIFConfig:
    """CDIF í•™ìŠµ ì„¤ì •"""
    # ëª¨ë¸ ì„¤ì •
    max_waypoints: int = 8
    feature_dim: int = 256
    hidden_dim: int = 512
    num_layers: int = 6
    
    # í•™ìŠµ ì„¤ì •
    batch_size: int = 32
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    num_epochs: int = 100
    warmup_epochs: int = 5
    
    # Diffusion ì„¤ì •
    num_train_timesteps: int = 1000
    beta_schedule: str = "cosine"
    
    # ë°ì´í„° ì„¤ì •
    target_samples: int = 10000
    train_ratio: float = 0.9
    
    # GPU ì„¤ì •
    mixed_precision: bool = True
    gradient_clip: float = 1.0
    
    # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
    save_every: int = 5
    validate_every: int = 2
    early_stopping_patience: int = 15

class StrategicWaypointExtractor:
    """ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self, grid_size=0.2):
        self.grid_size = grid_size
        
    def extract_strategic_waypoints(self, start_pos, goal_pos, cost_map, max_waypoints=6):
        """ë³‘ëª©ì§€ì  ê¸°ë°˜ ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        # 1. A* ê¸°ë³¸ ê²½ë¡œ ìƒì„±
        simulator = RobotSimulator('scenarios/Circulation1.xml')
        astar_path = simulator.a_star(start_pos, goal_pos)
        
        if not astar_path or len(astar_path) < 2:
            return [start_pos, goal_pos]
        
        # 2. ê²½ë¡œë¥¼ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ ìƒì„±
        path_length = len(astar_path)
        if path_length <= max_waypoints:
            return astar_path
        
        # 3. ê· ë“± ê°„ê²©ìœ¼ë¡œ ì›¨ì´í¬ì¸íŠ¸ ì„ íƒ
        indices = np.linspace(0, path_length - 1, max_waypoints, dtype=int)
        strategic_waypoints = [astar_path[i] for i in indices]
        
        # 4. ì‹œì‘ì ê³¼ ëì  ë³´ì¥
        strategic_waypoints[0] = start_pos
        strategic_waypoints[-1] = goal_pos
        
        return strategic_waypoints

class CDIFDataset(Dataset):
    """CDIF í•™ìŠµ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_samples: List[Dict], config: CDIFConfig):
        self.samples = data_samples
        self.config = config
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ë°ì´í„° ë¡œë“œ (JSONì—ì„œ ë¡œë“œëœ ë¦¬ìŠ¤íŠ¸ë¥¼ NumPyë¡œ ë³€í™˜)
        cost_map = torch.from_numpy(np.array(sample['cost_map'])).float().unsqueeze(0)  # [1, 60, 60]
        start_pos = torch.from_numpy(np.array(sample['start_pos'])).float()  # [2]
        goal_pos = torch.from_numpy(np.array(sample['goal_pos'])).float()  # [2]
        waypoints = np.array(sample['strategic_waypoints'])  # Convert to numpy array
        
        # ì›¨ì´í¬ì¸íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ë¡œ íŒ¨ë”©
        max_waypoints = self.config.max_waypoints
        num_waypoints = len(waypoints)
        
        if num_waypoints > max_waypoints:
            # ë„ˆë¬´ ë§ìœ¼ë©´ ì˜ë¼ë‚´ê¸°
            waypoints = waypoints[:max_waypoints]
            num_waypoints = max_waypoints
        else:
            # ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ì ìœ¼ë¡œ íŒ¨ë”©
            waypoints_padded = waypoints.tolist()
            while len(waypoints_padded) < max_waypoints:
                waypoints_padded.append(waypoints_padded[-1])
            waypoints = np.array(waypoints_padded)
        
        waypoints_tensor = torch.from_numpy(waypoints).float()  # [max_waypoints, 2]
        num_waypoints_tensor = torch.tensor(num_waypoints, dtype=torch.long)
        
        return {
            'cost_map': cost_map,
            'start_pos': start_pos,
            'goal_pos': goal_pos,
            'waypoints': waypoints_tensor,
            'num_waypoints': num_waypoints_tensor
        }

class CDIFDataCollector:
    """CDIF ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: CDIFConfig, output_dir='training_data_cdif'):
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
        self.simulator = RobotSimulator('scenarios/Circulation1.xml')
        self.extractor = StrategicWaypointExtractor()
        
    def collect_data(self) -> List[Dict]:
        """ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ¯ ëª©í‘œ: {self.config.target_samples:,}ê°œ CDIF ë°ì´í„° ìˆ˜ì§‘")
        
        collected_data = []
        
        # ì•ˆì „í•œ ìœ„ì¹˜ ìƒì„±ì„ ìœ„í•œ ì˜ì—­ ì •ì˜
        safe_zones = [
            (-5.5, -5.5, -0.5, -0.5),  # ì¢Œí•˜
            (-5.5, 0.5, -0.5, 5.5),    # ì¢Œìƒ
            (0.5, -5.5, 5.5, -0.5),    # ìš°í•˜
            (0.5, 0.5, 5.5, 5.5),      # ìš°ìƒ
            (-2.0, -2.0, 2.0, 2.0)     # ì¤‘ì•™
        ]
        
        pbar = tqdm(total=self.config.target_samples, desc="CDIF ë°ì´í„° ìˆ˜ì§‘")
        
        while len(collected_data) < self.config.target_samples:
            # ëœë¤ ì‹œì‘/ëª©í‘œì  ìƒì„±
            start_pos = self._generate_safe_position(safe_zones)
            goal_pos = self._generate_safe_position(safe_zones)
            
            if start_pos is None or goal_pos is None:
                continue
            
            # ê±°ë¦¬ ì²´í¬ (ìµœì†Œ 3m ì´ìƒ)
            dist = np.linalg.norm(np.array(start_pos) - np.array(goal_pos))
            if dist < 3.0:
                continue
            
            # ì „ëµì  ì›¨ì´í¬ì¸íŠ¸ ì¶”ì¶œ
            strategic_waypoints = self.extractor.extract_strategic_waypoints(
                start_pos, goal_pos, self.simulator.fused_cost_map, 
                max_waypoints=self.config.max_waypoints
            )
            
            if len(strategic_waypoints) >= 2:
                # ë°ì´í„° ì •ê·œí™” (-6~6 â†’ -1~1)
                data_sample = {
                    'cost_map': self.simulator.fused_cost_map.copy(),
                    'start_pos': np.array([start_pos[0]/6.0, start_pos[1]/6.0]),
                    'goal_pos': np.array([goal_pos[0]/6.0, goal_pos[1]/6.0]),
                    'strategic_waypoints': np.array([[p[0]/6.0, p[1]/6.0] for p in strategic_waypoints]),
                    'num_waypoints': len(strategic_waypoints)
                }
                
                collected_data.append(data_sample)
                pbar.update(1)
        
        pbar.close()
        
        # ë°ì´í„° ì €ì¥
        save_path = self.output_dir / 'cdif_training_data.json'
        with open(save_path, 'w') as f:
            json.dump(collected_data, f, cls=NumpyEncoder, indent=2)
        
        print(f"âœ… CDIF ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data):,}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {save_path}")
        
        return collected_data
    
    def _generate_safe_position(self, safe_zones):
        """ì•ˆì „í•œ ìœ„ì¹˜ ìƒì„±"""
        for _ in range(50):
            zone = random.choice(safe_zones)
            x = random.uniform(zone[0], zone[2])
            y = random.uniform(zone[1], zone[3])
            
            # ê·¸ë¦¬ë“œ ê¸°ë°˜ ì•ˆì „ì„± ì²´í¬
            x_idx = int((x + 6) / self.simulator.grid_size)
            y_idx = int((y + 6) / self.simulator.grid_size)
            
            if (0 <= x_idx < 60 and 0 <= y_idx < 60 and 
                self.simulator.grid[y_idx, x_idx] == 0):
                return [x, y]
        
        return None

class NumpyEncoder(json.JSONEncoder):
    """NumPy ë°°ì—´ì„ JSONìœ¼ë¡œ ì¸ì½”ë”©"""
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

class CDIFTrainer:
    """CDIF í•™ìŠµê¸°"""
    
    def __init__(self, config: CDIFConfig, output_dir='models'):
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        self.model = CDIFModel(
            max_waypoints=config.max_waypoints,
            feature_dim=config.feature_dim,
            hidden_dim=config.hidden_dim,
            num_layers=config.num_layers
        ).to(self.device)
        
        self.scheduler = DDPMScheduler(
            num_train_timesteps=config.num_train_timesteps,
            beta_schedule=config.beta_schedule
        ).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        self.lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2
        )
        
        # Mixed Precision ì„¤ì •
        self.scaler = torch.cuda.amp.GradScaler() if config.mixed_precision else None
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=f'runs/cdif_{int(time.time())}')
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def train(self, train_data: List[Dict], val_data: List[Dict]):
        """CDIF ëª¨ë¸ í•™ìŠµ"""
        print("ğŸ“ CDIF í•™ìŠµ ì‹œì‘!")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = CDIFDataset(train_data, self.config)
        val_dataset = CDIFDataset(val_data, self.config)
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_dataset):,}ê°œ")
        print(f"ğŸ“– ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ")
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(self.config.num_epochs):
            # í•™ìŠµ
            train_loss = self._train_epoch(train_loader, epoch)
            
            # ê²€ì¦
            if epoch % self.config.validate_every == 0:
                val_loss = self._validate_epoch(val_loader, epoch)
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.patience_counter = 0
                    self._save_checkpoint(epoch, is_best=True)
                    print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Val Loss: {val_loss:.6f}")
                else:
                    self.patience_counter += 1
                
                # Early stopping
                if self.patience_counter >= self.config.early_stopping_patience:
                    print(f"â° Early stopping at epoch {epoch+1}")
                    break
            
            # ì •ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if epoch % self.config.save_every == 0:
                self._save_checkpoint(epoch)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            self.lr_scheduler.step()
        
        print("âœ… CDIF í•™ìŠµ ì™„ë£Œ!")
        self.writer.close()
    
    def _train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training")
        
        for batch_idx, batch in enumerate(pbar):
            # ë°ì´í„° GPUë¡œ ì´ë™
            cost_map = batch['cost_map'].to(self.device)  # [B, 1, 60, 60]
            start_pos = batch['start_pos'].to(self.device)  # [B, 2]
            goal_pos = batch['goal_pos'].to(self.device)  # [B, 2]
            waypoints = batch['waypoints'].to(self.device)  # [B, max_waypoints, 2]
            num_waypoints = batch['num_waypoints'].to(self.device)  # [B]
            
            batch_size = cost_map.shape[0]
            
            # ëœë¤ íƒ€ì„ìŠ¤í… ìƒì„±
            timesteps = torch.randint(
                0, self.config.num_train_timesteps, 
                (batch_size,), device=self.device
            )
            
            # ì¡ìŒ ìƒì„±
            noise = torch.randn_like(waypoints)
            
            # ì¡ìŒ ì¶”ê°€
            noisy_waypoints = self.scheduler.add_noise(waypoints, noise, timesteps)
            
            # ìˆœì „íŒŒ
            with torch.cuda.amp.autocast(enabled=self.config.mixed_precision):
                predicted_noise, num_waypoints_prob = self.model(
                    cost_map, noisy_waypoints, timesteps, start_pos, goal_pos
                )
                
                # ì†ì‹¤ ê³„ì‚°
                noise_loss = F.mse_loss(predicted_noise, noise)
                
                # ì›¨ì´í¬ì¸íŠ¸ ìˆ˜ ì˜ˆì¸¡ ì†ì‹¤
                num_targets = F.one_hot(num_waypoints - 1, num_classes=self.config.max_waypoints).float()
                num_loss = F.cross_entropy(num_waypoints_prob, num_targets.argmax(dim=1))
                
                total_loss_batch = noise_loss + 0.1 * num_loss
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            
            if self.scaler:
                self.scaler.scale(total_loss_batch).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                total_loss_batch.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.optimizer.step()
            
            # ì†ì‹¤ ëˆ„ì 
            total_loss += total_loss_batch.item()
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'Loss': f"{total_loss_batch.item():.6f}",
                'Noise': f"{noise_loss.item():.6f}",
                'Num': f"{num_loss.item():.6f}"
            })
            
            # TensorBoard ë¡œê¹…
            if self.global_step % 100 == 0:
                self.writer.add_scalar('Train/Loss', total_loss_batch.item(), self.global_step)
                self.writer.add_scalar('Train/NoiseLoss', noise_loss.item(), self.global_step)
                self.writer.add_scalar('Train/NumLoss', num_loss.item(), self.global_step)
                self.writer.add_scalar('Train/LR', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            self.global_step += 1
        
        avg_loss = total_loss / len(train_loader)
        return avg_loss
    
    def _validate_epoch(self, val_loader: DataLoader, epoch: int) -> float:
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1} Validation"):
                # ë°ì´í„° GPUë¡œ ì´ë™
                cost_map = batch['cost_map'].to(self.device)
                start_pos = batch['start_pos'].to(self.device)
                goal_pos = batch['goal_pos'].to(self.device)
                waypoints = batch['waypoints'].to(self.device)
                num_waypoints = batch['num_waypoints'].to(self.device)
                
                batch_size = cost_map.shape[0]
                
                # ëœë¤ íƒ€ì„ìŠ¤í…
                timesteps = torch.randint(
                    0, self.config.num_train_timesteps,
                    (batch_size,), device=self.device
                )
                
                # ì¡ìŒ ì¶”ê°€
                noise = torch.randn_like(waypoints)
                noisy_waypoints = self.scheduler.add_noise(waypoints, noise, timesteps)
                
                # ìˆœì „íŒŒ
                predicted_noise, num_waypoints_prob = self.model(
                    cost_map, noisy_waypoints, timesteps, start_pos, goal_pos
                )
                
                # ì†ì‹¤ ê³„ì‚°
                noise_loss = F.mse_loss(predicted_noise, noise)
                num_targets = F.one_hot(num_waypoints - 1, num_classes=self.config.max_waypoints).float()
                num_loss = F.cross_entropy(num_waypoints_prob, num_targets.argmax(dim=1))
                
                total_loss_batch = noise_loss + 0.1 * num_loss
                total_loss += total_loss_batch.item()
        
        avg_loss = total_loss / len(val_loader)
        
        # TensorBoard ë¡œê¹…
        self.writer.add_scalar('Val/Loss', avg_loss, epoch)
        
        print(f"ğŸ“Š Epoch {epoch+1} - Val Loss: {avg_loss:.6f}")
        
        return avg_loss
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.lr_scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'global_step': self.global_step
        }
        
        if is_best:
            save_path = self.output_dir / 'cdif_best.pth'
            torch.save(checkpoint, save_path)
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {save_path}")
        
        # ì •ê¸°ì  ì²´í¬í¬ì¸íŠ¸
        save_path = self.output_dir / f'cdif_epoch_{epoch+1}.pth'
        torch.save(checkpoint, save_path)

def main():
    parser = argparse.ArgumentParser(description='CDIF Training')
    parser.add_argument('--config', type=str, help='Config file path')
    parser.add_argument('--data_only', action='store_true', help='Only collect data')
    parser.add_argument('--resume', type=str, help='Resume from checkpoint')
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = CDIFConfig()
    
    print("ğŸš€ CDIF í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    print(f"âš™ï¸  ì„¤ì •: {config}")
    
    # 1. ë°ì´í„° ìˆ˜ì§‘
    collector = CDIFDataCollector(config)
    
    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    data_path = collector.output_dir / 'cdif_training_data.json'
    if data_path.exists():
        print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {data_path}")
        with open(data_path, 'r') as f:
            all_data = json.load(f)
    else:
        print("ğŸ“Š ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        all_data = collector.collect_data()
    
    if args.data_only:
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ë§Œ ì™„ë£Œ!")
        return
    
    # 2. ë°ì´í„° ë¶„í• 
    random.shuffle(all_data)
    split_idx = int(len(all_data) * config.train_ratio)
    train_data = all_data[:split_idx]
    val_data = all_data[split_idx:]
    
    print(f"ğŸ“š í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• : {len(train_data)}/{len(val_data)}")
    
    # 3. í•™ìŠµ
    trainer = CDIFTrainer(config)
    
    if args.resume:
        print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {args.resume}")
        checkpoint = torch.load(args.resume)
        trainer.model.load_state_dict(checkpoint['model_state_dict'])
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        trainer.lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        trainer.best_val_loss = checkpoint['best_val_loss']
        trainer.global_step = checkpoint['global_step']
    
    trainer.train(train_data, val_data)

if __name__ == "__main__":
    main() 